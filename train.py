{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14552852,"datasetId":9295140,"databundleVersionId":15383824},{"sourceType":"kernelVersion","sourceId":293575169},{"sourceType":"kernelVersion","sourceId":293579780}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# imports\nimport random\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchmetrics.audio import ScaleInvariantSignalNoiseRatio\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom dataset import AudioDataset\nfrom model import ConvTasNet\nimport warnings\n\n# set config\nconfig = {\n    'metadata_dir': Path('/kaggle/input/heart-lung-source-separation/data/metadata'), # path to folder containing all metadata files\n    'audio_dir': Path('/kaggle/input/heart-lung-source-separation/data/audio'), # path to folder containing audio files\n    'train_samples_per_epoch': 1024, # number of training samples to generate (on-the-fly) per epoch\n    'val_samples_per_epoch': 512, # number of validation samples to generate (on-the-fly) per epoch\n    'target_sample_rate': 16000,\n    'num_samples': 80000,\n    'batch_size': 32,\n    'epochs': 1,\n    'N': 128, # number of filters in autoencoder\n    'L': 40, # length of the filters (in samples)\n    'B': 128, # number of channels in the bottleneck and residual paths' 1x1-conv blocks\n    'H': 256, # number of channels in conv blocks\n    'Sc': 128, # number of channels in skip-connections paths' 1x1-conv blocks\n    'P': 3, # kernel size in conv blocks\n    'R': 2, # number of repeats\n    'X': 7, # number of conv blocks in each repeat\n    'lr': 3e-4, # learning rate\n    'dropout': 0.0, # dropout probability\n    'weight_decay': 0.01, # weight decay for AdamW\n    'max_norm': float('inf'), # max norm for gradient clipping\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu', # set device\n    'seed': 123\n}\n\ndef get_ds(config):\n    # datasets\n    training_set = AudioDataset(config['metadata_dir'] / 'train.csv', config['audio_dir'], config['train_samples_per_epoch'], config['target_sample_rate'], config['num_samples'])\n    validation_set = AudioDataset(config['metadata_dir'] / 'val.csv', config['audio_dir'], config['val_samples_per_epoch'], config['target_sample_rate'], config['num_samples'], deterministic = True)\n\n    # dataloaders\n    train_loader = DataLoader(training_set, config['batch_size'], shuffle = True)\n    val_loader = DataLoader(validation_set, config['batch_size'], shuffle = False)\n\n    return train_loader, val_loader\n\ndef get_model(config):\n    model = ConvTasNet(N = config['N'], L = config['L'], B = config['B'], H = config['H'], Sc = config['Sc'], P = config['P'], R = config['R'], X = config['X'], dropout = config['dropout'])\n    return model\n\ndef train_and_validate(config):\n    # set device\n    device = config['device']\n\n    # get dataloaders\n    train_loader, val_loader = get_ds(config)\n\n    # get model and move to device\n    model = get_model(config).to(device)\n\n    # set loss function and move to device\n    criterion = ScaleInvariantSignalNoiseRatio().to(device)\n\n    # set optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr = config['lr'], weight_decay = config['weight_decay'])\n\n    # iterate over epochs\n    for epoch in tqdm(range(config['epochs'])):\n        \n        # initialize training and validation losses\n        train_loss = 0.0\n        val_loss = 0.0\n        \n        # training loop\n        model.train()\n        for data in tqdm(train_loader, desc = 'Training...', leave = False):\n            # move data to device\n            heart_signal, lung_signal, mixed_signal = data['target_heart'], data['target_lung'], data['mixture']\n            heart_signal, lung_signal, mixed_signal = heart_signal.to(device), lung_signal.to(device), mixed_signal.to(device)\n\n            # clear gradients\n            optimizer.zero_grad()\n\n            # get predictions\n            preds = model(mixed_signal) # (batch_size, 2, L)\n            preds = preds.view(-1, heart_signal.size(-1)) # (batch_size * 2, L)\n\n            # targets\n            targets = torch.concat([heart_signal, lung_signal], dim = 1).view(-1, preds.size(-1))\n            \n            # compute loss (negate to minimize)\n            loss = -criterion(preds, targets)\n    \n            # backprop \n            loss.backward()\n    \n            # compute total gradient norm across all parameters\n            # total_norm = 0.0\n            # for param in model.parameters():\n            #     if param.grad is not None:\n            #         total_norm += param.grad.data.norm(2).item()**2\n            # total_norm = total_norm**0.5\n    \n            # update total epoch grad norm\n            # epoch_grad_norm += total_norm\n            \n            # update weights\n            optimizer.step()\n    \n            # update total loss over this epoch\n            train_loss += loss.item()\n        train_loss = train_loss / len(train_loader)\n\n        # validation loop\n        model.eval()\n        # avoid gradient computations\n        with torch.no_grad():\n    \n            # iterate over batches\n            for data in tqdm(val_loader, desc = 'Validating...', leave = False):\n    \n                # read in and move data to device\n                heart_signal, lung_signal, mixed_signal = data['target_heart'], data['target_lung'], data['mixture']\n                heart_signal, lung_signal, mixed_signal = heart_signal.to(device), lung_signal.to(device), mixed_signal.to(device)\n                \n                # get predictions\n                preds = model(mixed_signal)\n                preds = preds.view(-1, heart_signal.size(-1)) # (batch_size * 2, L)\n                \n                # targets\n                targets = torch.concat([heart_signal, lung_signal], dim = 1).view(-1, preds.size(-1))\n                \n                # compute average loss over batch\n                batch_loss = -criterion(preds, targets)\n    \n                # update total epoch loss\n                val_loss += batch_loss.item()\n            \n            # compute avg loss for this epoch\n            val_loss = val_loss / len(val_loader)\n    \n            # compute si-snr\n            si_snr = -val_loss\n\n        print(f'Epoch {epoch}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}')\n\nif __name__ == '__main__':\n    warnings.filterwarnings('ignore')\n    train_and_validate(config)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-23T23:33:33.894956Z","iopub.execute_input":"2026-01-23T23:33:33.895806Z","iopub.status.idle":"2026-01-23T23:34:55.511819Z","shell.execute_reply.started":"2026-01-23T23:33:33.895773Z","shell.execute_reply":"2026-01-23T23:34:55.511101Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]\nTraining...:   0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\nTraining...:   3%|▎         | 1/32 [00:05<02:58,  5.77s/it]\u001b[A\nTraining...:   6%|▋         | 2/32 [00:07<01:50,  3.68s/it]\u001b[A\nTraining...:   9%|▉         | 3/32 [00:10<01:25,  2.95s/it]\u001b[A\nTraining...:  12%|█▎        | 4/32 [00:12<01:11,  2.56s/it]\u001b[A\nTraining...:  16%|█▌        | 5/32 [00:13<01:02,  2.32s/it]\u001b[A\nTraining...:  19%|█▉        | 6/32 [00:15<00:56,  2.16s/it]\u001b[A\nTraining...:  22%|██▏       | 7/32 [00:17<00:51,  2.05s/it]\u001b[A\nTraining...:  25%|██▌       | 8/32 [00:19<00:47,  1.98s/it]\u001b[A\nTraining...:  28%|██▊       | 9/32 [00:21<00:44,  1.91s/it]\u001b[A\nTraining...:  31%|███▏      | 10/32 [00:23<00:41,  1.88s/it]\u001b[A\nTraining...:  34%|███▍      | 11/32 [00:24<00:39,  1.86s/it]\u001b[A\nTraining...:  38%|███▊      | 12/32 [00:26<00:36,  1.85s/it]\u001b[A\nTraining...:  41%|████      | 13/32 [00:28<00:34,  1.83s/it]\u001b[A\nTraining...:  44%|████▍     | 14/32 [00:30<00:32,  1.80s/it]\u001b[A\nTraining...:  47%|████▋     | 15/32 [00:31<00:30,  1.78s/it]\u001b[A\nTraining...:  50%|█████     | 16/32 [00:33<00:28,  1.78s/it]\u001b[A\nTraining...:  53%|█████▎    | 17/32 [00:35<00:26,  1.76s/it]\u001b[A\nTraining...:  56%|█████▋    | 18/32 [00:37<00:24,  1.75s/it]\u001b[A\nTraining...:  59%|█████▉    | 19/32 [00:38<00:22,  1.75s/it]\u001b[A\nTraining...:  62%|██████▎   | 20/32 [00:40<00:21,  1.76s/it]\u001b[A\nTraining...:  66%|██████▌   | 21/32 [00:42<00:19,  1.76s/it]\u001b[A\nTraining...:  69%|██████▉   | 22/32 [00:44<00:17,  1.77s/it]\u001b[A\nTraining...:  72%|███████▏  | 23/32 [00:45<00:15,  1.77s/it]\u001b[A\nTraining...:  75%|███████▌  | 24/32 [00:47<00:14,  1.79s/it]\u001b[A\nTraining...:  78%|███████▊  | 25/32 [00:49<00:12,  1.80s/it]\u001b[A\nTraining...:  81%|████████▏ | 26/32 [00:51<00:10,  1.78s/it]\u001b[A\nTraining...:  84%|████████▍ | 27/32 [00:53<00:08,  1.79s/it]\u001b[A\nTraining...:  88%|████████▊ | 28/32 [00:54<00:07,  1.78s/it]\u001b[A\nTraining...:  91%|█████████ | 29/32 [00:56<00:05,  1.77s/it]\u001b[A\nTraining...:  94%|█████████▍| 30/32 [00:58<00:03,  1.77s/it]\u001b[A\nTraining...:  97%|█████████▋| 31/32 [01:00<00:01,  1.77s/it]\u001b[A\nTraining...: 100%|██████████| 32/32 [01:01<00:00,  1.77s/it]\u001b[A\n                                                            \u001b[A\nValidating...:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\nValidating...:   6%|▋         | 1/16 [00:01<00:21,  1.45s/it]\u001b[A\nValidating...:  12%|█▎        | 2/16 [00:02<00:18,  1.30s/it]\u001b[A\nValidating...:  19%|█▉        | 3/16 [00:03<00:16,  1.26s/it]\u001b[A\nValidating...:  25%|██▌       | 4/16 [00:05<00:14,  1.22s/it]\u001b[A\nValidating...:  31%|███▏      | 5/16 [00:06<00:13,  1.21s/it]\u001b[A\nValidating...:  38%|███▊      | 6/16 [00:07<00:11,  1.19s/it]\u001b[A\nValidating...:  44%|████▍     | 7/16 [00:08<00:10,  1.19s/it]\u001b[A\nValidating...:  50%|█████     | 8/16 [00:09<00:09,  1.19s/it]\u001b[A\nValidating...:  56%|█████▋    | 9/16 [00:10<00:08,  1.19s/it]\u001b[A\nValidating...:  62%|██████▎   | 10/16 [00:12<00:07,  1.19s/it]\u001b[A\nValidating...:  69%|██████▉   | 11/16 [00:13<00:05,  1.19s/it]\u001b[A\nValidating...:  75%|███████▌  | 12/16 [00:14<00:04,  1.20s/it]\u001b[A\nValidating...:  81%|████████▏ | 13/16 [00:15<00:03,  1.22s/it]\u001b[A\nValidating...:  88%|████████▊ | 14/16 [00:17<00:02,  1.24s/it]\u001b[A\nValidating...:  94%|█████████▍| 15/16 [00:18<00:01,  1.23s/it]\u001b[A\nValidating...: 100%|██████████| 16/16 [00:19<00:00,  1.23s/it]\u001b[A\n100%|██████████| 1/1 [01:21<00:00, 81.55s/it]                 \u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 0: Train Loss = 4.605415, Val Loss = 1.219018\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9}]}